step 0: train loss 10.8468, val loss 10.8475, train perplexity 51367.1953, val perplexity 51407.4297
Decreasing dropout to 0.0000900000 due to decreased validation loss.
New best validation loss: 10.847537994384766.
Current learning rate: 0.0005999999999962989
iter 0: loss 21.7227, time 52941.66ms, mfu -100.00%
Current learning rate: 0.0005999999995521667
iter 10: loss 20.0479, time 583.07ms, mfu 0.05%
Current learning rate: 0.0005999999983678141
iter 20: loss 18.4838, time 583.27ms, mfu 0.05%
Current learning rate: 0.0005999999964432413
iter 30: loss 17.3823, time 583.97ms, mfu 0.05%
Current learning rate: 0.0005999999937784484
iter 40: loss 16.2992, time 584.02ms, mfu 0.05%
Current learning rate: 0.0005999999903734349
iter 50: loss 16.5203, time 583.61ms, mfu 0.05%
Current learning rate: 0.000599999986228201
iter 60: loss 14.6601, time 583.65ms, mfu 0.05%
Current learning rate: 0.0005999999813427468
iter 70: loss 14.9406, time 583.52ms, mfu 0.05%
Current learning rate: 0.0005999999757170727
iter 80: loss 15.2348, time 583.82ms, mfu 0.05%
Current learning rate: 0.000599999969351178
iter 90: loss 14.2531, time 584.44ms, mfu 0.05%
Current learning rate: 0.000599999962245063
iter 100: loss 14.0631, time 584.19ms, mfu 0.05%
Current learning rate: 0.000599999954398728
iter 110: loss 14.8007, time 585.11ms, mfu 0.05%
Current learning rate: 0.0005999999458121728
iter 120: loss 14.2188, time 583.93ms, mfu 0.05%
Current learning rate: 0.000599999936485397
iter 130: loss 14.7067, time 583.97ms, mfu 0.05%
Current learning rate: 0.0005999999264184012
iter 140: loss 14.3467, time 585.22ms, mfu 0.05%
Current learning rate: 0.0005999999156111852
iter 150: loss 14.0919, time 584.17ms, mfu 0.05%
Current learning rate: 0.0005999999040637494
iter 160: loss 14.3979, time 585.85ms, mfu 0.05%
Current learning rate: 0.0005999998917760934
iter 170: loss 13.3405, time 584.08ms, mfu 0.05%
Current learning rate: 0.0005999998787482176
iter 180: loss 13.5644, time 585.34ms, mfu 0.05%
Current learning rate: 0.0005999998649801217
iter 190: loss 13.7353, time 584.37ms, mfu 0.05%
step 200: train loss 6.9735, val loss 7.0328, train perplexity 1067.9930, val perplexity 1133.2555
Decreasing dropout to 0.0000810000 due to decreased validation loss.
New best validation loss: 7.0328497886657715.
Current learning rate: 0.0005999998504718059
iter 200: loss 13.4683, time 17404.92ms, mfu 0.05%
Current learning rate: 0.00059999983522327
iter 210: loss 13.1765, time 584.40ms, mfu 0.05%
Current learning rate: 0.0005999998192345143
iter 220: loss 13.4211, time 584.49ms, mfu 0.05%
Current learning rate: 0.000599999802505539
iter 230: loss 13.7385, time 584.48ms, mfu 0.05%
Current learning rate: 0.0005999997850363436
iter 240: loss 13.3467, time 585.67ms, mfu 0.05%
Current learning rate: 0.0005999997668269287
iter 250: loss 14.0065, time 583.79ms, mfu 0.05%
Current learning rate: 0.000599999747877294
iter 260: loss 12.7575, time 585.40ms, mfu 0.05%
Current learning rate: 0.0005999997281874397
iter 270: loss 13.8910, time 584.05ms, mfu 0.05%
Current learning rate: 0.0005999997077573658
iter 280: loss 12.6132, time 585.20ms, mfu 0.05%
Current learning rate: 0.0005999996865870725
iter 290: loss 13.0511, time 585.99ms, mfu 0.05%
Current learning rate: 0.0005999996646765594
iter 300: loss 12.9245, time 584.29ms, mfu 0.05%
Current learning rate: 0.0005999996420258271
iter 310: loss 12.3245, time 584.94ms, mfu 0.05%
Current learning rate: 0.0005999996186348752
iter 320: loss 14.0022, time 584.13ms, mfu 0.05%
Current learning rate: 0.0005999995945037042
iter 330: loss 12.9911, time 585.79ms, mfu 0.05%
Current learning rate: 0.0005999995696323139
iter 340: loss 14.6506, time 583.94ms, mfu 0.05%
Current learning rate: 0.0005999995440207044
iter 350: loss 13.9859, time 585.19ms, mfu 0.05%
Current learning rate: 0.0005999995176688757
iter 360: loss 12.9908, time 584.12ms, mfu 0.05%
Current learning rate: 0.0005999994905768281
iter 370: loss 13.0887, time 584.65ms, mfu 0.05%
Current learning rate: 0.0005999994627445617
iter 380: loss 12.9550, time 585.83ms, mfu 0.05%
Current learning rate: 0.0005999994341720764
iter 390: loss 13.4695, time 584.12ms, mfu 0.05%
step 400: train loss 6.7446, val loss 6.7293, train perplexity 849.4766, val perplexity 836.5240
Decreasing dropout to 0.0000729000 due to decreased validation loss.
New best validation loss: 6.729255199432373.
Current learning rate: 0.0005999994048593722
iter 400: loss 12.6550, time 17659.24ms, mfu 0.05%
Current learning rate: 0.0005999993748064493
iter 410: loss 13.5757, time 584.31ms, mfu 0.05%
Current learning rate: 0.0005999993440133078
iter 420: loss 14.4490, time 585.59ms, mfu 0.05%
Current learning rate: 0.0005999993124799478
iter 430: loss 13.0046, time 584.60ms, mfu 0.05%
Current learning rate: 0.0005999992802063694
iter 440: loss 12.9928, time 585.97ms, mfu 0.05%
Current learning rate: 0.0005999992471925729
iter 450: loss 14.0483, time 584.01ms, mfu 0.05%
Current learning rate: 0.000599999213438558
iter 460: loss 14.4834, time 585.59ms, mfu 0.05%
Current learning rate: 0.000599999178944325
iter 470: loss 13.1730, time 586.29ms, mfu 0.05%
Current learning rate: 0.0005999991437098736
iter 480: loss 12.3561, time 584.19ms, mfu 0.05%
Current learning rate: 0.0005999991077352045
iter 490: loss 13.0491, time 588.49ms, mfu 0.05%
Current learning rate: 0.0005999990710203173
iter 500: loss 13.7000, time 588.55ms, mfu 0.05%
Current learning rate: 0.0005999990335652124
iter 510: loss 12.3797, time 589.43ms, mfu 0.05%
Current learning rate: 0.0005999989953698899
iter 520: loss 12.7991, time 589.83ms, mfu 0.05%
Current learning rate: 0.0005999989564343498
iter 530: loss 12.9070, time 589.29ms, mfu 0.05%
Current learning rate: 0.0005999989167585923
iter 540: loss 12.3198, time 587.95ms, mfu 0.05%
Current learning rate: 0.0005999988763426176
iter 550: loss 12.3653, time 589.90ms, mfu 0.05%
Current learning rate: 0.0005999988351864255
iter 560: loss 13.1766, time 589.90ms, mfu 0.05%
Current learning rate: 0.000599998793290016
iter 570: loss 12.4061, time 589.33ms, mfu 0.05%
Current learning rate: 0.0005999987506533895
iter 580: loss 13.2774, time 588.56ms, mfu 0.05%
Current learning rate: 0.0005999987072765463
iter 590: loss 12.6173, time 589.00ms, mfu 0.05%
step 600: train loss 6.5999, val loss 6.5996, train perplexity 734.9998, val perplexity 734.8253
Decreasing dropout to 0.0000656100 due to decreased validation loss.
New best validation loss: 6.599632740020752.
Current learning rate: 0.000599998663159486
iter 600: loss 11.7225, time 17810.11ms, mfu 0.05%
Current learning rate: 0.0005999986183022094
iter 610: loss 12.5380, time 589.17ms, mfu 0.05%
Current learning rate: 0.0005999985727047161
iter 620: loss 12.7168, time 589.16ms, mfu 0.05%
Current learning rate: 0.0005999985263670062
iter 630: loss 13.5097, time 589.66ms, mfu 0.05%
Current learning rate: 0.0005999984792890801
iter 640: loss 13.0325, time 589.81ms, mfu 0.05%
Current learning rate: 0.000599998431470938
iter 650: loss 12.7281, time 590.21ms, mfu 0.05%
Current learning rate: 0.0005999983829125798
iter 660: loss 13.0152, time 589.49ms, mfu 0.05%
Current learning rate: 0.0005999983336140058
iter 670: loss 12.5241, time 589.22ms, mfu 0.05%
Current learning rate: 0.0005999982835752162
iter 680: loss 12.8625, time 588.28ms, mfu 0.05%
Current learning rate: 0.0005999982327962107
iter 690: loss 12.9569, time 588.62ms, mfu 0.05%
Current learning rate: 0.0005999981812769898
iter 700: loss 12.5100, time 588.89ms, mfu 0.05%
Current learning rate: 0.0005999981290175537
iter 710: loss 12.2954, time 590.44ms, mfu 0.05%
Current learning rate: 0.0005999980760179021
iter 720: loss 12.0749, time 590.30ms, mfu 0.05%
Current learning rate: 0.0005999980222780355
iter 730: loss 13.2296, time 589.99ms, mfu 0.05%
Current learning rate: 0.000599997967797954
iter 740: loss 14.4510, time 589.99ms, mfu 0.05%
Current learning rate: 0.0005999979125776579
iter 750: loss 13.0696, time 589.75ms, mfu 0.05%
Current learning rate: 0.0005999978566171472
iter 760: loss 12.8021, time 589.14ms, mfu 0.05%
Current learning rate: 0.0005999977999164217
iter 770: loss 13.6127, time 588.86ms, mfu 0.05%
Current learning rate: 0.000599997742475482
iter 780: loss 12.6315, time 589.34ms, mfu 0.05%
Current learning rate: 0.0005999976842943282
iter 790: loss 12.2505, time 590.01ms, mfu 0.05%
step 800: train loss 6.6505, val loss 6.6056, train perplexity 773.2024, val perplexity 739.2437
Increasing dropout to 0.0000721710 due to increased validation loss.
Current learning rate: 0.0005999976253729607
iter 800: loss 13.8523, time 17817.78ms, mfu 0.05%
Current learning rate: 0.0005999975657113791
iter 810: loss 11.4783, time 589.99ms, mfu 0.05%
Current learning rate: 0.000599997505309584
iter 820: loss 12.0237, time 589.02ms, mfu 0.05%
Current learning rate: 0.0005999974441675751
iter 830: loss 11.4726, time 588.89ms, mfu 0.05%
Current learning rate: 0.0005999973822853532
iter 840: loss 12.2590, time 589.78ms, mfu 0.05%
Current learning rate: 0.0005999973196629181
iter 850: loss 11.5263, time 590.49ms, mfu 0.05%
Current learning rate: 0.0005999972563002699
iter 860: loss 12.2199, time 590.08ms, mfu 0.05%
Current learning rate: 0.0005999971921974088
iter 870: loss 11.8251, time 590.37ms, mfu 0.05%
Current learning rate: 0.0005999971273543353
iter 880: loss 12.5857, time 590.27ms, mfu 0.05%
Current learning rate: 0.0005999970617710491
iter 890: loss 11.8929, time 590.11ms, mfu 0.05%
Current learning rate: 0.0005999969954475507
iter 900: loss 11.9439, time 589.63ms, mfu 0.05%
Current learning rate: 0.0005999969283838404
iter 910: loss 11.3392, time 589.14ms, mfu 0.05%
Current learning rate: 0.0005999968605799181
iter 920: loss 11.3992, time 589.24ms, mfu 0.05%
Current learning rate: 0.0005999967920357841
iter 930: loss 11.9014, time 590.29ms, mfu 0.05%
Current learning rate: 0.0005999967227514386
iter 940: loss 10.9522, time 590.22ms, mfu 0.05%
Current learning rate: 0.0005999966527268815
iter 950: loss 12.5441, time 590.02ms, mfu 0.05%
Current learning rate: 0.0005999965819621135
iter 960: loss 11.3151, time 589.89ms, mfu 0.05%
Current learning rate: 0.0005999965104571347
iter 970: loss 11.9008, time 588.54ms, mfu 0.05%
Current learning rate: 0.0005999964382119448
iter 980: loss 11.0344, time 588.91ms, mfu 0.05%
Current learning rate: 0.0005999963652265444
iter 990: loss 11.8331, time 589.26ms, mfu 0.05%
step 1000: train loss 6.5805, val loss 6.6042, train perplexity 720.8747, val perplexity 738.1718
Increasing dropout to 0.0000793881 due to increased validation loss.
Saving checkpoint to out_tiny, with current dropout rate: 7.938810000000003e-05.
Current learning rate: 0.0005999962915009335
iter 1000: loss 11.2759, time 17934.43ms, mfu 0.05%
Current learning rate: 0.0005999962170351127
iter 1010: loss 10.9167, time 589.91ms, mfu 0.05%
Current learning rate: 0.0005999961418290818
iter 1020: loss 11.4557, time 589.97ms, mfu 0.05%
Current learning rate: 0.0005999960658828414
iter 1030: loss 11.1083, time 590.25ms, mfu 0.05%
Current learning rate: 0.0005999959891963912
iter 1040: loss 10.2429, time 589.34ms, mfu 0.05%
Current learning rate: 0.000599995911769732
iter 1050: loss 11.0165, time 589.30ms, mfu 0.05%
Current learning rate: 0.0005999958336028634
iter 1060: loss 11.7684, time 589.77ms, mfu 0.05%
Current learning rate: 0.0005999957546957859
iter 1070: loss 11.9209, time 589.08ms, mfu 0.05%
Current learning rate: 0.0005999956750484996
iter 1080: loss 9.7510, time 589.20ms, mfu 0.05%
Current learning rate: 0.0005999955946610052
iter 1090: loss 11.7508, time 589.03ms, mfu 0.05%
Current learning rate: 0.0005999955135333022
iter 1100: loss 9.4593, time 588.76ms, mfu 0.05%
Current learning rate: 0.0005999954316653913
iter 1110: loss 11.5495, time 588.87ms, mfu 0.05%
Current learning rate: 0.0005999953490572726
iter 1120: loss 11.1327, time 589.20ms, mfu 0.05%
Current learning rate: 0.0005999952657089464
iter 1130: loss 10.7124, time 589.54ms, mfu 0.05%
Current learning rate: 0.0005999951816204126
iter 1140: loss 10.3683, time 589.26ms, mfu 0.05%
Current learning rate: 0.0005999950967916718
iter 1150: loss 9.6609, time 590.15ms, mfu 0.05%
Current learning rate: 0.0005999950112227242
iter 1160: loss 10.6679, time 589.98ms, mfu 0.05%
Current learning rate: 0.0005999949249135698
iter 1170: loss 10.6259, time 590.33ms, mfu 0.05%
Current learning rate: 0.0005999948378642092
iter 1180: loss 11.0076, time 590.18ms, mfu 0.05%
Current learning rate: 0.0005999947500746421
iter 1190: loss 10.5601, time 590.84ms, mfu 0.05%
step 1200: train loss 6.5066, val loss 6.5407, train perplexity 669.5620, val perplexity 692.7729
Decreasing dropout to 0.0000714493 due to decreased validation loss.
New best validation loss: 6.5407023429870605.
Current learning rate: 0.0005999946615448693
iter 1200: loss 11.1649, time 17789.26ms, mfu 0.05%
Current learning rate: 0.0005999945722748906
iter 1210: loss 10.2932, time 589.85ms, mfu 0.05%
Current learning rate: 0.0005999944822647068
iter 1220: loss 10.6268, time 590.37ms, mfu 0.05%
Current learning rate: 0.0005999943915143176
iter 1230: loss 11.4372, time 589.46ms, mfu 0.05%
Current learning rate: 0.0005999943000237236
iter 1240: loss 13.2390, time 588.77ms, mfu 0.05%
Current learning rate: 0.0005999942077929246
iter 1250: loss 11.7554, time 588.34ms, mfu 0.05%
Current learning rate: 0.0005999941148219217
iter 1260: loss 10.5705, time 589.38ms, mfu 0.05%
Current learning rate: 0.000599994021110714
iter 1270: loss 11.2620, time 589.16ms, mfu 0.05%
Current learning rate: 0.0005999939266593027
iter 1280: loss 10.1281, time 590.81ms, mfu 0.05%
Current learning rate: 0.0005999938314676877
iter 1290: loss 12.4402, time 589.96ms, mfu 0.05%
Current learning rate: 0.000599993735535869
iter 1300: loss 10.5125, time 590.29ms, mfu 0.05%
Current learning rate: 0.0005999936388638475
iter 1310: loss 10.1091, time 590.25ms, mfu 0.05%
Current learning rate: 0.0005999935414516231
iter 1320: loss 10.5872, time 589.66ms, mfu 0.05%
Current learning rate: 0.0005999934432991958
iter 1330: loss 10.4170, time 590.13ms, mfu 0.05%
Current learning rate: 0.0005999933444065661
iter 1340: loss 10.6415, time 590.48ms, mfu 0.05%
Current learning rate: 0.0005999932447737344
iter 1350: loss 10.3735, time 590.32ms, mfu 0.05%
Current learning rate: 0.0005999931444007011
iter 1360: loss 9.9930, time 589.91ms, mfu 0.05%
Current learning rate: 0.000599993043287466
iter 1370: loss 11.8124, time 589.88ms, mfu 0.05%
Current learning rate: 0.0005999929414340298
iter 1380: loss 10.7424, time 589.89ms, mfu 0.05%
Current learning rate: 0.0005999928388403926
iter 1390: loss 10.6763, time 590.18ms, mfu 0.05%
step 1400: train loss 6.4909, val loss 6.5092, train perplexity 659.1069, val perplexity 671.2927
Decreasing dropout to 0.0000643044 due to decreased validation loss.
New best validation loss: 6.509205341339111.
Current learning rate: 0.0005999927355065548
iter 1400: loss 9.2711, time 17770.47ms, mfu 0.05%
Current learning rate: 0.0005999926314325164
iter 1410: loss 10.0311, time 590.15ms, mfu 0.05%
Current learning rate: 0.0005999925266182781
iter 1420: loss 9.6946, time 589.71ms, mfu 0.05%
Current learning rate: 0.0005999924210638398
iter 1430: loss 8.8048, time 590.12ms, mfu 0.05%
Current learning rate: 0.0005999923147692017
iter 1440: loss 10.2375, time 590.04ms, mfu 0.05%
Current learning rate: 0.0005999922077343647
iter 1450: loss 10.1492, time 590.09ms, mfu 0.05%
Current learning rate: 0.0005999920999593286
iter 1460: loss 10.7953, time 590.01ms, mfu 0.05%
Current learning rate: 0.0005999919914440937
iter 1470: loss 9.3336, time 590.76ms, mfu 0.05%
Current learning rate: 0.0005999918821886606
iter 1480: loss 9.1791, time 590.56ms, mfu 0.05%
Current learning rate: 0.0005999917721930295
iter 1490: loss 10.1906, time 591.03ms, mfu 0.05%
Current learning rate: 0.0005999916614572006
iter 1500: loss 9.9020, time 590.41ms, mfu 0.05%
Current learning rate: 0.0005999915499811742
iter 1510: loss 10.2665, time 590.30ms, mfu 0.05%
Current learning rate: 0.0005999914377649507
iter 1520: loss 9.3211, time 589.91ms, mfu 0.05%
Current learning rate: 0.0005999913248085304
iter 1530: loss 9.0247, time 589.58ms, mfu 0.05%
Current learning rate: 0.0005999912111119132
iter 1540: loss 9.3942, time 590.22ms, mfu 0.05%
Current learning rate: 0.0005999910966751
iter 1550: loss 8.9829, time 590.43ms, mfu 0.05%
Current learning rate: 0.0005999909814980908
iter 1560: loss 9.2401, time 590.32ms, mfu 0.05%
Current learning rate: 0.0005999908655808859
iter 1570: loss 10.1318, time 590.37ms, mfu 0.05%
Current learning rate: 0.0005999907489234856
iter 1580: loss 10.1925, time 590.47ms, mfu 0.05%
Current learning rate: 0.0005999906315258906
iter 1590: loss 9.9764, time 590.47ms, mfu 0.05%
step 1600: train loss 6.3868, val loss 6.4109, train perplexity 593.9590, val perplexity 608.4562
Decreasing dropout to 0.0000578739 due to decreased validation loss.
New best validation loss: 6.410924911499023.
Current learning rate: 0.0005999905133881009
iter 1600: loss 9.0084, time 17761.26ms, mfu 0.05%
Current learning rate: 0.0005999903945101171
iter 1610: loss 9.2206, time 590.01ms, mfu 0.05%
Current learning rate: 0.000599990274891939
iter 1620: loss 8.8135, time 589.95ms, mfu 0.05%
Current learning rate: 0.0005999901545335673
iter 1630: loss 10.3152, time 589.89ms, mfu 0.05%
Current learning rate: 0.0005999900334350021
iter 1640: loss 9.5877, time 589.55ms, mfu 0.05%
Current learning rate: 0.0005999899115962441
iter 1650: loss 10.1445, time 589.67ms, mfu 0.05%
Current learning rate: 0.0005999897890172938
iter 1660: loss 11.0307, time 589.12ms, mfu 0.05%
Current learning rate: 0.0005999896656981508
iter 1670: loss 9.5385, time 588.58ms, mfu 0.05%
Current learning rate: 0.0005999895416388159
iter 1680: loss 9.2344, time 588.86ms, mfu 0.05%
Current learning rate: 0.0005999894168392892
iter 1690: loss 10.3235, time 589.14ms, mfu 0.05%
Current learning rate: 0.0005999892912995713
iter 1700: loss 10.4024, time 588.83ms, mfu 0.05%
Current learning rate: 0.0005999891650196623
iter 1710: loss 9.3912, time 588.95ms, mfu 0.05%
Current learning rate: 0.0005999890379995629
iter 1720: loss 9.3025, time 589.37ms, mfu 0.05%
Current learning rate: 0.0005999889102392733
iter 1730: loss 9.7282, time 588.73ms, mfu 0.05%
Current learning rate: 0.0005999887817387935
iter 1740: loss 9.9933, time 589.35ms, mfu 0.05%
Current learning rate: 0.0005999886524981241
iter 1750: loss 9.5017, time 589.64ms, mfu 0.05%
Current learning rate: 0.0005999885225172657
iter 1760: loss 9.5707, time 589.98ms, mfu 0.05%
Current learning rate: 0.0005999883917962181
iter 1770: loss 10.2202, time 590.54ms, mfu 0.05%
Current learning rate: 0.0005999882603349821
iter 1780: loss 9.4027, time 591.10ms, mfu 0.05%
Current learning rate: 0.0005999881281335581
iter 1790: loss 9.7609, time 589.49ms, mfu 0.05%
step 1800: train loss 6.3405, val loss 6.3510, train perplexity 567.0621, val perplexity 573.0731
Decreasing dropout to 0.0000520865 due to decreased validation loss.
New best validation loss: 6.35101318359375.
Current learning rate: 0.0005999879951919461
iter 1800: loss 9.2971, time 17805.02ms, mfu 0.05%
Current learning rate: 0.0005999878615101469
iter 1810: loss 9.4837, time 590.16ms, mfu 0.05%
Current learning rate: 0.0005999877270881605
iter 1820: loss 9.4984, time 590.02ms, mfu 0.05%
Current learning rate: 0.0005999875919259875
iter 1830: loss 9.0070, time 590.45ms, mfu 0.05%
Current learning rate: 0.0005999874560236282
iter 1840: loss 9.8927, time 590.07ms, mfu 0.05%
Current learning rate: 0.0005999873193810828
iter 1850: loss 10.7908, time 590.49ms, mfu 0.05%
Current learning rate: 0.0005999871819983519
iter 1860: loss 8.1818, time 590.15ms, mfu 0.05%
Current learning rate: 0.000599987043875436
iter 1870: loss 10.1099, time 590.47ms, mfu 0.05%
Current learning rate: 0.0005999869050123353
iter 1880: loss 8.8003, time 589.86ms, mfu 0.05%
Current learning rate: 0.0005999867654090501
iter 1890: loss 8.2264, time 590.13ms, mfu 0.05%
Current learning rate: 0.0005999866250655809
iter 1900: loss 11.0333, time 590.12ms, mfu 0.05%
Current learning rate: 0.0005999864839819282
iter 1910: loss 9.2551, time 590.05ms, mfu 0.05%
Current learning rate: 0.0005999863421580921
iter 1920: loss 9.3016, time 589.36ms, mfu 0.05%
Current learning rate: 0.0005999861995940729
iter 1930: loss 9.5542, time 589.81ms, mfu 0.05%
Current learning rate: 0.0005999860562898711
iter 1940: loss 8.9766, time 588.98ms, mfu 0.05%
Current learning rate: 0.0005999859122454873
iter 1950: loss 9.2003, time 588.95ms, mfu 0.05%
Current learning rate: 0.0005999857674609221
iter 1960: loss 8.6425, time 589.04ms, mfu 0.05%
Current learning rate: 0.0005999856219361753
iter 1970: loss 8.5842, time 589.74ms, mfu 0.05%
Current learning rate: 0.0005999854756712475
iter 1980: loss 9.3093, time 590.22ms, mfu 0.05%
Current learning rate: 0.0005999853286661395
iter 1990: loss 8.5276, time 589.90ms, mfu 0.05%
step 2000: train loss 6.3627, val loss 6.3675, train perplexity 579.7950, val perplexity 582.6219
Increasing dropout to 0.0000572952 due to increased validation loss.
Saving checkpoint to out_tiny, with current dropout rate: 5.7295185651000025e-05.
Current learning rate: 0.000599985180920851
iter 2000: loss 9.2222, time 17985.75ms, mfu 0.05%
Current learning rate: 0.0005999850324353832
iter 2010: loss 11.8392, time 589.73ms, mfu 0.05%
Current learning rate: 0.0005999848832097358
iter 2020: loss 7.8475, time 590.19ms, mfu 0.05%
Current learning rate: 0.0005999847332439097
iter 2030: loss 8.8561, time 589.76ms, mfu 0.05%
Current learning rate: 0.0005999845825379052
iter 2040: loss 9.5316, time 588.85ms, mfu 0.05%
Current learning rate: 0.0005999844310917225
iter 2050: loss 10.3309, time 588.80ms, mfu 0.05%
Current learning rate: 0.0005999842789053622
iter 2060: loss 9.0267, time 588.99ms, mfu 0.05%
Current learning rate: 0.0005999841259788245
iter 2070: loss 9.1033, time 588.93ms, mfu 0.05%
Current learning rate: 0.0005999839723121102
iter 2080: loss 8.1882, time 589.44ms, mfu 0.05%
Current learning rate: 0.0005999838179052196
iter 2090: loss 8.8531, time 589.84ms, mfu 0.05%
Current learning rate: 0.0005999836627581527
iter 2100: loss 8.8259, time 590.33ms, mfu 0.05%
Current learning rate: 0.0005999835068709105
iter 2110: loss 9.1999, time 589.73ms, mfu 0.05%
Current learning rate: 0.0005999833502434931
iter 2120: loss 8.6237, time 590.30ms, mfu 0.05%
Current learning rate: 0.000599983192875901
iter 2130: loss 8.0419, time 589.90ms, mfu 0.05%
Current learning rate: 0.0005999830347681348
iter 2140: loss 10.3645, time 590.20ms, mfu 0.05%
Current learning rate: 0.0005999828759201945
iter 2150: loss 9.4570, time 589.79ms, mfu 0.05%
Current learning rate: 0.0005999827163320811
iter 2160: loss 8.9075, time 589.95ms, mfu 0.05%
Current learning rate: 0.0005999825560037945
iter 2170: loss 9.1145, time 590.13ms, mfu 0.05%
Current learning rate: 0.0005999823949353355
iter 2180: loss 8.2103, time 589.99ms, mfu 0.05%
Current learning rate: 0.0005999822331267046
iter 2190: loss 8.6317, time 590.02ms, mfu 0.05%
step 2200: train loss 6.3119, val loss 6.3102, train perplexity 551.0972, val perplexity 550.1443
Decreasing dropout to 0.0000515657 due to decreased validation loss.
New best validation loss: 6.3101806640625.
Current learning rate: 0.0005999820705779018
iter 2200: loss 10.1340, time 17779.87ms, mfu 0.05%
Current learning rate: 0.0005999819072889279
iter 2210: loss 8.9856, time 588.84ms, mfu 0.05%
Current learning rate: 0.0005999817432597832
iter 2220: loss 8.3647, time 588.91ms, mfu 0.05%
Current learning rate: 0.0005999815784904683
iter 2230: loss 10.8522, time 589.02ms, mfu 0.05%
Current learning rate: 0.0005999814129809837
iter 2240: loss 8.2304, time 589.54ms, mfu 0.05%
Current learning rate: 0.0005999812467313297
iter 2250: loss 8.7288, time 589.39ms, mfu 0.05%
Current learning rate: 0.0005999810797415067
iter 2260: loss 9.5425, time 589.77ms, mfu 0.05%
Current learning rate: 0.0005999809120115154
iter 2270: loss 9.5338, time 590.68ms, mfu 0.05%
Current learning rate: 0.0005999807435413559
iter 2280: loss 8.5017, time 590.45ms, mfu 0.05%
Current learning rate: 0.000599980574331029
iter 2290: loss 8.9065, time 590.03ms, mfu 0.05%
Current learning rate: 0.000599980404380535
iter 2300: loss 8.7430, time 589.96ms, mfu 0.05%
Current learning rate: 0.0005999802336898745
iter 2310: loss 9.3761, time 590.77ms, mfu 0.05%
Current learning rate: 0.0005999800622590478
iter 2320: loss 9.7412, time 590.47ms, mfu 0.05%
Current learning rate: 0.0005999798900880554
iter 2330: loss 8.5282, time 589.62ms, mfu 0.05%
Current learning rate: 0.000599979717176898
iter 2340: loss 8.3839, time 589.76ms, mfu 0.05%
Current learning rate: 0.0005999795435255754
iter 2350: loss 8.4214, time 589.82ms, mfu 0.05%
Current learning rate: 0.0005999793691340888
iter 2360: loss 7.9718, time 589.52ms, mfu 0.05%
Current learning rate: 0.0005999791940024383
iter 2370: loss 8.9550, time 590.02ms, mfu 0.05%
Current learning rate: 0.0005999790181306248
iter 2380: loss 8.6632, time 589.80ms, mfu 0.05%
Current learning rate: 0.0005999788415186482
iter 2390: loss 9.6245, time 589.48ms, mfu 0.05%
step 2400: train loss 6.3377, val loss 6.3038, train perplexity 565.4995, val perplexity 546.6649
Decreasing dropout to 0.0000464091 due to decreased validation loss.
New best validation loss: 6.303835868835449.
Current learning rate: 0.0005999786641665094
iter 2400: loss 8.3049, time 17763.41ms, mfu 0.05%
Current learning rate: 0.0005999784860742087
iter 2410: loss 8.7349, time 590.78ms, mfu 0.05%
Current learning rate: 0.0005999783072417468
iter 2420: loss 8.6568, time 590.02ms, mfu 0.05%
Current learning rate: 0.0005999781276691241
iter 2430: loss 8.9744, time 588.98ms, mfu 0.05%
Current learning rate: 0.000599977947356341
iter 2440: loss 8.8888, time 588.53ms, mfu 0.05%
Current learning rate: 0.0005999777663033978
iter 2450: loss 8.7161, time 589.13ms, mfu 0.05%
Current learning rate: 0.0005999775845102952
iter 2460: loss 8.0080, time 588.96ms, mfu 0.05%
Current learning rate: 0.0005999774019770339
iter 2470: loss 8.5881, time 590.13ms, mfu 0.05%
Current learning rate: 0.0005999772187036142
iter 2480: loss 8.1542, time 590.06ms, mfu 0.05%
Current learning rate: 0.0005999770346900363
iter 2490: loss 9.0328, time 589.96ms, mfu 0.05%
Current learning rate: 0.0005999768499363014
iter 2500: loss 9.5692, time 589.90ms, mfu 0.05%
Current learning rate: 0.0005999766644424094
iter 2510: loss 8.7587, time 590.26ms, mfu 0.05%
Current learning rate: 0.0005999764782083613
iter 2520: loss 9.8484, time 594.20ms, mfu 0.05%
Current learning rate: 0.0005999762912341571
iter 2530: loss 8.6944, time 593.96ms, mfu 0.05%
Current learning rate: 0.0005999761035197978
iter 2540: loss 8.5128, time 593.83ms, mfu 0.05%
Current learning rate: 0.0005999759150652835
iter 2550: loss 8.2401, time 594.50ms, mfu 0.05%
Current learning rate: 0.000599975725870615
iter 2560: loss 8.6998, time 594.17ms, mfu 0.05%
Current learning rate: 0.0005999755359357927
iter 2570: loss 8.4717, time 593.53ms, mfu 0.05%
Current learning rate: 0.0005999753452608172
iter 2580: loss 8.7205, time 593.32ms, mfu 0.05%
Current learning rate: 0.000599975153845689
iter 2590: loss 8.1212, time 593.37ms, mfu 0.05%
step 2600: train loss 6.1910, val loss 6.3014, train perplexity 488.3451, val perplexity 545.3212
Decreasing dropout to 0.0000417682 due to decreased validation loss.
New best validation loss: 6.301374912261963.
Current learning rate: 0.0005999749616904085
iter 2600: loss 8.1771, time 17829.83ms, mfu 0.05%
Current learning rate: 0.0005999747687949762
iter 2610: loss 8.6981, time 593.92ms, mfu 0.05%
Current learning rate: 0.000599974575159393
iter 2620: loss 8.8302, time 593.04ms, mfu 0.05%
Current learning rate: 0.0005999743807836591
iter 2630: loss 8.4416, time 593.30ms, mfu 0.05%
Current learning rate: 0.000599974185667775
iter 2640: loss 9.2354, time 593.55ms, mfu 0.05%
Current learning rate: 0.0005999739898117415
iter 2650: loss 8.9503, time 592.80ms, mfu 0.05%
Current learning rate: 0.0005999737932155592
iter 2660: loss 9.0509, time 593.06ms, mfu 0.05%
Current learning rate: 0.0005999735958792281
iter 2670: loss 8.7942, time 593.53ms, mfu 0.05%
Current learning rate: 0.0005999733978027489
iter 2680: loss 8.3224, time 593.77ms, mfu 0.05%
Current learning rate: 0.0005999731989861223
iter 2690: loss 8.7998, time 593.18ms, mfu 0.05%
Current learning rate: 0.0005999729994293492
iter 2700: loss 8.6030, time 593.53ms, mfu 0.05%
Current learning rate: 0.0005999727991324298
iter 2710: loss 9.0913, time 593.41ms, mfu 0.05%
Current learning rate: 0.0005999725980953645
iter 2720: loss 9.2483, time 593.99ms, mfu 0.05%
Current learning rate: 0.000599972396318154
iter 2730: loss 8.7721, time 594.23ms, mfu 0.05%
Current learning rate: 0.0005999721938007987
iter 2740: loss 8.5810, time 593.68ms, mfu 0.05%
Current learning rate: 0.0005999719905432995
iter 2750: loss 8.5110, time 593.48ms, mfu 0.05%
Current learning rate: 0.0005999717865456567
iter 2760: loss 8.4732, time 594.27ms, mfu 0.05%
Current learning rate: 0.0005999715818078709
iter 2770: loss 8.1906, time 594.26ms, mfu 0.05%
Current learning rate: 0.0005999713763299427
iter 2780: loss 8.5778, time 594.36ms, mfu 0.05%
Current learning rate: 0.0005999711701118726
iter 2790: loss 8.5416, time 594.30ms, mfu 0.05%
step 2800: train loss 6.2266, val loss 6.1833, train perplexity 506.0085, val perplexity 484.5963
Decreasing dropout to 0.0000375914 due to decreased validation loss.
New best validation loss: 6.183316230773926.
Current learning rate: 0.0005999709631536611
iter 2800: loss 8.1671, time 17833.15ms, mfu 0.05%
Current learning rate: 0.0005999707554553089
iter 2810: loss 8.9074, time 592.71ms, mfu 0.05%
Current learning rate: 0.0005999705470168167
iter 2820: loss 8.5953, time 593.09ms, mfu 0.05%
Current learning rate: 0.000599970337838185
iter 2830: loss 8.5441, time 593.91ms, mfu 0.05%
Current learning rate: 0.000599970127919414
iter 2840: loss 9.6351, time 594.15ms, mfu 0.05%
Current learning rate: 0.0005999699172605049
iter 2850: loss 8.7404, time 594.24ms, mfu 0.05%
Current learning rate: 0.0005999697058614576
iter 2860: loss 10.0036, time 594.63ms, mfu 0.05%
Current learning rate: 0.0005999694937222729
iter 2870: loss 8.7069, time 593.87ms, mfu 0.05%
Current learning rate: 0.000599969280842952
iter 2880: loss 8.6561, time 594.17ms, mfu 0.05%
Current learning rate: 0.0005999690672234947
iter 2890: loss 8.9780, time 593.77ms, mfu 0.05%
Current learning rate: 0.0005999688528639019
iter 2900: loss 7.7691, time 594.18ms, mfu 0.05%
Current learning rate: 0.0005999686377641742
iter 2910: loss 8.4096, time 593.72ms, mfu 0.05%
Current learning rate: 0.0005999684219243123
iter 2920: loss 8.9130, time 593.54ms, mfu 0.05%
Current learning rate: 0.0005999682053443165
iter 2930: loss 8.3192, time 593.96ms, mfu 0.05%
Current learning rate: 0.0005999679880241878
iter 2940: loss 8.1707, time 594.18ms, mfu 0.05%
Current learning rate: 0.0005999677699639264
iter 2950: loss 8.7049, time 593.90ms, mfu 0.05%
Current learning rate: 0.0005999675511635328
iter 2960: loss 7.7272, time 594.27ms, mfu 0.05%
Current learning rate: 0.0005999673316230078
iter 2970: loss 7.1902, time 594.17ms, mfu 0.05%
Current learning rate: 0.0005999671113423523
iter 2980: loss 8.9522, time 594.71ms, mfu 0.05%
Current learning rate: 0.0005999668903215664
iter 2990: loss 9.4336, time 594.49ms, mfu 0.05%
step 3000: train loss 6.2408, val loss 6.2008, train perplexity 513.2643, val perplexity 493.1354
Increasing dropout to 0.0000413505 due to increased validation loss.
Saving checkpoint to out_tiny, with current dropout rate: 4.135050843618324e-05.
Current learning rate: 0.0005999666685606513
iter 3000: loss 7.6978, time 18133.87ms, mfu 0.05%
Current learning rate: 0.0005999664460596068
iter 3010: loss 9.5245, time 593.89ms, mfu 0.05%
Current learning rate: 0.0005999662228184341
iter 3020: loss 8.4441, time 594.45ms, mfu 0.05%
Current learning rate: 0.0005999659988371337
iter 3030: loss 8.5081, time 593.55ms, mfu 0.05%
Current learning rate: 0.0005999657741157064
iter 3040: loss 7.8652, time 593.66ms, mfu 0.05%
Current learning rate: 0.0005999655486541524
iter 3050: loss 8.4310, time 593.90ms, mfu 0.05%
Current learning rate: 0.0005999653224524724
iter 3060: loss 8.2306, time 593.82ms, mfu 0.05%
Current learning rate: 0.0005999650955106672
iter 3070: loss 8.1118, time 594.40ms, mfu 0.05%
Current learning rate: 0.0005999648678287376
iter 3080: loss 7.6912, time 593.75ms, mfu 0.05%
Current learning rate: 0.0005999646394066838
iter 3090: loss 8.2615, time 594.27ms, mfu 0.05%
Current learning rate: 0.0005999644102445065
iter 3100: loss 9.5010, time 593.48ms, mfu 0.05%
Current learning rate: 0.0005999641803422065
iter 3110: loss 7.6239, time 593.65ms, mfu 0.05%
Current learning rate: 0.0005999639496997844
iter 3120: loss 8.2524, time 594.64ms, mfu 0.05%
Current learning rate: 0.0005999637183172408
iter 3130: loss 9.4102, time 594.21ms, mfu 0.05%
Current learning rate: 0.0005999634861945765
iter 3140: loss 8.4432, time 593.89ms, mfu 0.05%
Current learning rate: 0.0005999632533317918
iter 3150: loss 7.7983, time 594.34ms, mfu 0.05%
Current learning rate: 0.0005999630197288874
iter 3160: loss 7.5979, time 593.85ms, mfu 0.05%
Current learning rate: 0.0005999627853858644
iter 3170: loss 8.2267, time 593.99ms, mfu 0.05%
Current learning rate: 0.0005999625503027229
iter 3180: loss 9.4627, time 593.74ms, mfu 0.05%
Current learning rate: 0.0005999623144794637
iter 3190: loss 8.5219, time 593.65ms, mfu 0.05%
step 3200: train loss 6.1710, val loss 6.1868, train perplexity 478.6743, val perplexity 486.3116
Increasing dropout to 0.0000454856 due to increased validation loss.
Current learning rate: 0.0005999620779160878
iter 3200: loss 8.0728, time 17883.86ms, mfu 0.05%
Current learning rate: 0.0005999618406125955
iter 3210: loss 7.5149, time 593.02ms, mfu 0.05%
Current learning rate: 0.0005999616025689873
iter 3220: loss 8.2776, time 593.76ms, mfu 0.05%
Current learning rate: 0.000599961363785264
iter 3230: loss 8.3509, time 593.76ms, mfu 0.05%
Current learning rate: 0.0005999611242614264
iter 3240: loss 8.5356, time 593.48ms, mfu 0.05%
Current learning rate: 0.0005999608839974749
iter 3250: loss 8.4893, time 594.55ms, mfu 0.05%
Current learning rate: 0.0005999606429934108
iter 3260: loss 8.7518, time 594.04ms, mfu 0.05%
Current learning rate: 0.0005999604012492338
iter 3270: loss 8.0131, time 593.86ms, mfu 0.05%
Current learning rate: 0.0005999601587649453
iter 3280: loss 8.4748, time 594.00ms, mfu 0.05%
Current learning rate: 0.0005999599155405452
iter 3290: loss 7.8987, time 593.76ms, mfu 0.05%
Current learning rate: 0.000599959671576035
iter 3300: loss 8.2049, time 593.79ms, mfu 0.05%
Current learning rate: 0.0005999594268714149
iter 3310: loss 8.9294, time 594.04ms, mfu 0.05%
Current learning rate: 0.0005999591814266858
iter 3320: loss 8.8005, time 593.72ms, mfu 0.05%
Current learning rate: 0.0005999589352418484
iter 3330: loss 7.5675, time 594.16ms, mfu 0.05%
Current learning rate: 0.0005999586883169031
iter 3340: loss 9.2071, time 594.46ms, mfu 0.05%
Current learning rate: 0.0005999584406518507
iter 3350: loss 8.7318, time 594.00ms, mfu 0.05%
Current learning rate: 0.0005999581922466918
iter 3360: loss 7.8158, time 594.41ms, mfu 0.05%
Current learning rate: 0.0005999579431014273
iter 3370: loss 8.8673, time 594.34ms, mfu 0.05%
Current learning rate: 0.0005999576932160579
iter 3380: loss 7.9016, time 594.50ms, mfu 0.05%
Current learning rate: 0.0005999574425905842
iter 3390: loss 7.3665, time 594.21ms, mfu 0.05%
step 3400: train loss 6.2189, val loss 6.1464, train perplexity 502.1260, val perplexity 467.0548
Decreasing dropout to 0.0000409370 due to decreased validation loss.
New best validation loss: 6.146446704864502.
Current learning rate: 0.0005999571912250067
iter 3400: loss 7.9207, time 17904.44ms, mfu 0.05%
Current learning rate: 0.000599956939119326
iter 3410: loss 8.5158, time 594.26ms, mfu 0.05%
Current learning rate: 0.0005999566862735432
iter 3420: loss 10.1548, time 593.09ms, mfu 0.05%
Current learning rate: 0.0005999564326876588
iter 3430: loss 8.7501, time 593.59ms, mfu 0.05%
Current learning rate: 0.0005999561783616738
iter 3440: loss 8.2161, time 593.46ms, mfu 0.05%
Current learning rate: 0.0005999559232955881
iter 3450: loss 9.4440, time 594.01ms, mfu 0.05%
Current learning rate: 0.0005999556674894034
iter 3460: loss 8.4951, time 594.33ms, mfu 0.05%
Current learning rate: 0.0005999554109431196
iter 3470: loss 8.6059, time 594.32ms, mfu 0.05%
Current learning rate: 0.0005999551536567378
iter 3480: loss 8.4899, time 594.41ms, mfu 0.05%
Current learning rate: 0.0005999548956302586
iter 3490: loss 8.4053, time 593.91ms, mfu 0.05%
Current learning rate: 0.0005999546368636826
iter 3500: loss 7.9692, time 594.34ms, mfu 0.05%
Current learning rate: 0.0005999543773570109
iter 3510: loss 8.1666, time 594.20ms, mfu 0.05%
Current learning rate: 0.0005999541171102439
iter 3520: loss 9.6297, time 593.28ms, mfu 0.05%
Current learning rate: 0.0005999538561233821
iter 3530: loss 7.7689, time 593.90ms, mfu 0.05%
Current learning rate: 0.0005999535943964268
iter 3540: loss 8.4613, time 594.01ms, mfu 0.05%
Current learning rate: 0.0005999533319293783
iter 3550: loss 8.0334, time 594.14ms, mfu 0.05%
Current learning rate: 0.0005999530687222376
iter 3560: loss 8.5845, time 594.22ms, mfu 0.05%
Current learning rate: 0.0005999528047750049
iter 3570: loss 7.9991, time 594.12ms, mfu 0.05%
Current learning rate: 0.0005999525400876813
iter 3580: loss 8.1157, time 594.44ms, mfu 0.05%
Current learning rate: 0.0005999522746602676
iter 3590: loss 7.2533, time 594.10ms, mfu 0.05%
step 3600: train loss 6.1150, val loss 6.1403, train perplexity 452.6052, val perplexity 464.2062
Decreasing dropout to 0.0000368433 due to decreased validation loss.
New best validation loss: 6.140328884124756.
Current learning rate: 0.0005999520084927643
iter 3600: loss 7.9368, time 17909.05ms, mfu 0.05%
Current learning rate: 0.0005999517415851721
iter 3610: loss 8.1548, time 594.31ms, mfu 0.05%
Current learning rate: 0.000599951473937492
iter 3620: loss 7.9544, time 594.35ms, mfu 0.05%
Current learning rate: 0.0005999512055497246
iter 3630: loss 7.0659, time 594.38ms, mfu 0.05%
Current learning rate: 0.0005999509364218707
iter 3640: loss 8.1480, time 594.13ms, mfu 0.05%
Current learning rate: 0.0005999506665539309
iter 3650: loss 8.3929, time 593.53ms, mfu 0.05%
Current learning rate: 0.0005999503959459061
iter 3660: loss 9.0300, time 594.33ms, mfu 0.05%
Current learning rate: 0.0005999501245977971
iter 3670: loss 8.9891, time 593.75ms, mfu 0.05%
Current learning rate: 0.0005999498525096044
iter 3680: loss 7.7641, time 594.05ms, mfu 0.05%
Current learning rate: 0.0005999495796813291
iter 3690: loss 8.0987, time 593.93ms, mfu 0.05%
Current learning rate: 0.0005999493061129712
iter 3700: loss 8.0707, time 593.99ms, mfu 0.05%
Current learning rate: 0.0005999490318045322
iter 3710: loss 8.3050, time 593.99ms, mfu 0.05%
Current learning rate: 0.0005999487567560128
iter 3720: loss 8.1025, time 594.51ms, mfu 0.05%
Current learning rate: 0.0005999484809674132
iter 3730: loss 9.2110, time 594.07ms, mfu 0.05%
Current learning rate: 0.0005999482044387347
iter 3740: loss 7.7235, time 594.24ms, mfu 0.05%
Current learning rate: 0.0005999479271699779
iter 3750: loss 7.5512, time 593.98ms, mfu 0.05%
Current learning rate: 0.0005999476491611435
iter 3760: loss 8.1841, time 593.39ms, mfu 0.05%
Current learning rate: 0.0005999473704122322
iter 3770: loss 7.8018, time 594.36ms, mfu 0.05%
Current learning rate: 0.0005999470909232452
iter 3780: loss 9.3673, time 593.67ms, mfu 0.05%
Current learning rate: 0.0005999468106941828
iter 3790: loss 8.4146, time 593.68ms, mfu 0.05%
step 3800: train loss 6.1952, val loss 6.1509, train perplexity 490.3652, val perplexity 469.1445
Increasing dropout to 0.0000405276 due to increased validation loss.
Current learning rate: 0.0005999465297250458
iter 3800: loss 8.4247, time 17783.71ms, mfu 0.05%
Current learning rate: 0.000599946248015835
iter 3810: loss 7.9810, time 593.23ms, mfu 0.05%
Current learning rate: 0.0005999459655665515
iter 3820: loss 8.0199, time 593.64ms, mfu 0.05%
Current learning rate: 0.0005999456823771955
iter 3830: loss 8.1535, time 593.55ms, mfu 0.05%
Current learning rate: 0.0005999453984477682
iter 3840: loss 8.3430, time 593.94ms, mfu 0.05%
Current learning rate: 0.0005999451137782703
iter 3850: loss 10.4386, time 594.01ms, mfu 0.05%
Current learning rate: 0.0005999448283687026
iter 3860: loss 8.7579, time 593.93ms, mfu 0.05%
Current learning rate: 0.0005999445422190658
iter 3870: loss 8.0790, time 593.17ms, mfu 0.05%
Current learning rate: 0.0005999442553293603
iter 3880: loss 8.0340, time 593.81ms, mfu 0.05%
Current learning rate: 0.0005999439676995878
iter 3890: loss 7.6859, time 593.65ms, mfu 0.05%
Current learning rate: 0.0005999436793297484
iter 3900: loss 7.5430, time 593.74ms, mfu 0.05%
Current learning rate: 0.000599943390219843
iter 3910: loss 8.0438, time 593.84ms, mfu 0.05%
Current learning rate: 0.0005999431003698726
iter 3920: loss 8.0607, time 594.05ms, mfu 0.05%
Current learning rate: 0.0005999428097798378
iter 3930: loss 9.1743, time 593.88ms, mfu 0.05%
Current learning rate: 0.0005999425184497394
iter 3940: loss 7.6849, time 593.55ms, mfu 0.05%
Current learning rate: 0.0005999422263795784
iter 3950: loss 8.8707, time 593.68ms, mfu 0.05%
Current learning rate: 0.0005999419335693555
iter 3960: loss 8.0842, time 593.68ms, mfu 0.05%
Current learning rate: 0.0005999416400190713
iter 3970: loss 8.2874, time 594.07ms, mfu 0.05%
Current learning rate: 0.0005999413457287267
iter 3980: loss 8.4030, time 594.17ms, mfu 0.05%
Current learning rate: 0.0005999410506983227
iter 3990: loss 7.3661, time 594.41ms, mfu 0.05%
step 4000: train loss 6.1163, val loss 6.1579, train perplexity 453.1652, val perplexity 472.4464
Increasing dropout to 0.0000445804 due to increased validation loss.
Saving checkpoint to out_tiny, with current dropout rate: 4.4580396650133525e-05.
Current learning rate: 0.00059994075492786
iter 4000: loss 8.7794, time 18082.51ms, mfu 0.05%
Current learning rate: 0.0005999404584173393
iter 4010: loss 8.5447, time 593.05ms, mfu 0.05%
Current learning rate: 0.0005999401611667614
iter 4020: loss 7.5740, time 592.95ms, mfu 0.05%
Current learning rate: 0.0005999398631761272
iter 4030: loss 8.4940, time 594.26ms, mfu 0.05%
Current learning rate: 0.0005999395644454378
iter 4040: loss 8.2264, time 594.37ms, mfu 0.05%
Current learning rate: 0.0005999392649746938
iter 4050: loss 8.0825, time 593.38ms, mfu 0.05%
Current learning rate: 0.0005999389647638959
iter 4060: loss 7.9773, time 594.02ms, mfu 0.05%
Current learning rate: 0.0005999386638130448
iter 4070: loss 7.7095, time 594.01ms, mfu 0.05%
Current learning rate: 0.0005999383621221417
iter 4080: loss 7.8724, time 593.64ms, mfu 0.05%
Current learning rate: 0.0005999380596911873
iter 4090: loss 8.4437, time 594.00ms, mfu 0.05%
Current learning rate: 0.0005999377565201823
iter 4100: loss 8.2673, time 593.06ms, mfu 0.05%
Current learning rate: 0.0005999374526091275
iter 4110: loss 8.5332, time 594.75ms, mfu 0.05%
Current learning rate: 0.0005999371479580239
iter 4120: loss 8.4491, time 593.86ms, mfu 0.05%
Current learning rate: 0.0005999368425668723
iter 4130: loss 7.6525, time 594.61ms, mfu 0.05%
Current learning rate: 0.0005999365364356737
iter 4140: loss 8.2049, time 593.88ms, mfu 0.05%
Current learning rate: 0.0005999362295644286
iter 4150: loss 8.1714, time 593.66ms, mfu 0.05%
Current learning rate: 0.000599935921953138
iter 4160: loss 8.5234, time 594.07ms, mfu 0.05%
Current learning rate: 0.0005999356136018028
iter 4170: loss 7.5604, time 593.13ms, mfu 0.05%
Current learning rate: 0.0005999353045104238
iter 4180: loss 8.7019, time 593.55ms, mfu 0.05%
Current learning rate: 0.0005999349946790019
iter 4190: loss 8.1413, time 593.66ms, mfu 0.05%
step 4200: train loss 6.1128, val loss 6.1639, train perplexity 451.5819, val perplexity 475.2834
Increasing dropout to 0.0000490384 due to increased validation loss.
Current learning rate: 0.0005999346841075379
iter 4200: loss 8.2400, time 17848.27ms, mfu 0.05%
Current learning rate: 0.0005999343727960325
iter 4210: loss 6.9106, time 593.49ms, mfu 0.05%
Current learning rate: 0.0005999340607444866
iter 4220: loss 8.2935, time 593.39ms, mfu 0.05%
Current learning rate: 0.0005999337479529013
iter 4230: loss 8.8244, time 593.87ms, mfu 0.05%
Current learning rate: 0.0005999334344212774
iter 4240: loss 7.5481, time 593.35ms, mfu 0.05%
Current learning rate: 0.0005999331201496154
iter 4250: loss 8.0957, time 593.11ms, mfu 0.05%
Current learning rate: 0.0005999328051379165
iter 4260: loss 8.1418, time 592.97ms, mfu 0.05%
Current learning rate: 0.0005999324893861816
iter 4270: loss 7.9509, time 594.08ms, mfu 0.05%
Current learning rate: 0.0005999321728944113
iter 4280: loss 7.7314, time 593.29ms, mfu 0.05%
Current learning rate: 0.0005999318556626067
iter 4290: loss 8.4774, time 593.83ms, mfu 0.05%
Current learning rate: 0.0005999315376907686
iter 4300: loss 7.3166, time 594.05ms, mfu 0.05%
Current learning rate: 0.0005999312189788979
iter 4310: loss 8.0086, time 594.55ms, mfu 0.05%
Current learning rate: 0.0005999308995269955
iter 4320: loss 9.4675, time 593.58ms, mfu 0.05%
Current learning rate: 0.000599930579335062
iter 4330: loss 9.3968, time 594.50ms, mfu 0.05%
Current learning rate: 0.0005999302584030988
iter 4340: loss 8.2755, time 594.18ms, mfu 0.05%
Current learning rate: 0.0005999299367311061
iter 4350: loss 8.3275, time 594.20ms, mfu 0.05%
Current learning rate: 0.0005999296143190852
iter 4360: loss 8.6412, time 593.58ms, mfu 0.05%
Current learning rate: 0.000599929291167037
iter 4370: loss 9.1336, time 594.26ms, mfu 0.05%
Current learning rate: 0.0005999289672749624
iter 4380: loss 7.4943, time 594.03ms, mfu 0.05%
Current learning rate: 0.0005999286426428622
iter 4390: loss 7.9351, time 593.82ms, mfu 0.05%
step 4400: train loss 6.1309, val loss 6.1204, train perplexity 459.8331, val perplexity 455.0681
Decreasing dropout to 0.0000441346 due to decreased validation loss.
New best validation loss: 6.120447158813477.
Current learning rate: 0.0005999283172707374
iter 4400: loss 8.1980, time 17909.07ms, mfu 0.05%
Current learning rate: 0.0005999279911585886
iter 4410: loss 8.3624, time 594.37ms, mfu 0.05%
Current learning rate: 0.0005999276643064173
iter 4420: loss 7.8509, time 594.17ms, mfu 0.05%
Current learning rate: 0.0005999273367142236
iter 4430: loss 8.4902, time 593.99ms, mfu 0.05%
Current learning rate: 0.0005999270083820089
iter 4440: loss 7.7758, time 594.22ms, mfu 0.05%
Current learning rate: 0.000599926679309774
iter 4450: loss 7.9314, time 593.88ms, mfu 0.05%
Current learning rate: 0.0005999263494975198
iter 4460: loss 7.7275, time 594.08ms, mfu 0.05%
Current learning rate: 0.0005999260189452469
iter 4470: loss 7.9217, time 594.24ms, mfu 0.05%
Current learning rate: 0.0005999256876529567
iter 4480: loss 6.6070, time 594.31ms, mfu 0.05%
Current learning rate: 0.0005999253556206497
iter 4490: loss 8.4404, time 593.68ms, mfu 0.05%
Current learning rate: 0.0005999250228483271
iter 4500: loss 7.6576, time 594.04ms, mfu 0.05%
Current learning rate: 0.00059992468933599
iter 4510: loss 8.6309, time 593.74ms, mfu 0.05%
Current learning rate: 0.000599924355083639
iter 4520: loss 8.7812, time 593.59ms, mfu 0.05%
Current learning rate: 0.0005999240200912748
iter 4530: loss 7.5610, time 593.65ms, mfu 0.05%
Current learning rate: 0.0005999236843588986
iter 4540: loss 7.7918, time 593.95ms, mfu 0.05%
Current learning rate: 0.0005999233478865111
iter 4550: loss 7.7611, time 593.84ms, mfu 0.05%
Current learning rate: 0.0005999230106741136
iter 4560: loss 7.2080, time 593.96ms, mfu 0.05%
Current learning rate: 0.000599922672721707
iter 4570: loss 7.9706, time 593.74ms, mfu 0.05%
Current learning rate: 0.0005999223340292919
iter 4580: loss 9.2248, time 593.96ms, mfu 0.05%
Current learning rate: 0.0005999219945968695
iter 4590: loss 8.3670, time 594.06ms, mfu 0.05%
step 4600: train loss 6.0858, val loss 6.1437, train perplexity 439.5666, val perplexity 465.7714
Increasing dropout to 0.0000485481 due to increased validation loss.
Current learning rate: 0.0005999216544244406
iter 4600: loss 9.7024, time 17881.85ms, mfu 0.05%
Current learning rate: 0.0005999213135120063
iter 4610: loss 8.6376, time 594.32ms, mfu 0.05%
Current learning rate: 0.0005999209718595674
iter 4620: loss 7.2934, time 594.00ms, mfu 0.05%
Current learning rate: 0.0005999206294671248
iter 4630: loss 8.4144, time 593.86ms, mfu 0.05%
Current learning rate: 0.0005999202863346795
iter 4640: loss 6.7109, time 594.47ms, mfu 0.05%
Traceback (most recent call last):
  File "/home/terry/WrkSpaces/LLM/myGpt/nanoGPTBiDirectional-dual/nanoGPTBiDirectional/train.py", line 379, in <module>
    forward_logits, forward_loss = model(X, Y, backward=False)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/terry/WrkSpaces/LLM/myGpt/nanoGPTBiDirectional-dual/nanoGPTBiDirectional/model.py", line 174, in forward
    def forward(self, idx, targets=None, backward=False):
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 3905, in forward
    return compiled_fn(full_args)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1482, in g
    return f(*args)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2527, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1506, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1482, in g
    return f(*args)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 3010, in forward
    fw_outs = call_func_with_args(
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1506, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 374, in __call__
    return self.get_current_callable()(inputs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 628, in run
    return model(new_inputs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 401, in _run_from_cache
    return compiled_graph.compiled_artifact(inputs)
  File "/tmp/torchinductor_terry/qw/cqw57ous45qmdpsvwfondgmggqp3ngx6l6g3svqu4qdzuitlk6er.py", line 1014, in call
    extern_kernels.mm(buf57, buf56, out=buf58)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/terry/WrkSpaces/LLM/myGpt/nanoGPTBiDirectional-dual/nanoGPTBiDirectional/train.py", line 379, in <module>
    forward_logits, forward_loss = model(X, Y, backward=False)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/terry/WrkSpaces/LLM/myGpt/nanoGPTBiDirectional-dual/nanoGPTBiDirectional/model.py", line 174, in forward
    def forward(self, idx, targets=None, backward=False):
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 3905, in forward
    return compiled_fn(full_args)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1482, in g
    return f(*args)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2527, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1506, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1482, in g
    return f(*args)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 3010, in forward
    fw_outs = call_func_with_args(
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1506, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 374, in __call__
    return self.get_current_callable()(inputs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 628, in run
    return model(new_inputs)
  File "/home/terry/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 401, in _run_from_cache
    return compiled_graph.compiled_artifact(inputs)
  File "/tmp/torchinductor_terry/qw/cqw57ous45qmdpsvwfondgmggqp3ngx6l6g3svqu4qdzuitlk6er.py", line 1014, in call
    extern_kernels.mm(buf57, buf56, out=buf58)
KeyboardInterrupt